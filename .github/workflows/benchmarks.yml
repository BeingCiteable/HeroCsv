name: Performance Benchmarks

on:
  pull_request:
    paths:
      - 'src/**'
      - 'benchmarks/**'
      - '.github/workflows/benchmarks.yml'
  push:
    branches: [master]
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: false
        default: 'quick'
        type: choice
        options:
          - quick
          - features
          - competitors
          - realdata
          - library
          - perf

env:
  DOTNET_VERSION: '9.0.x'
  DOTNET_NOLOGO: true
  DOTNET_CLI_TELEMETRY_OPTOUT: true

jobs:
  benchmark:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup .NET
      uses: actions/setup-dotnet@v4
      with:
        dotnet-version: ${{ env.DOTNET_VERSION }}
    
    - name: Restore dependencies
      run: dotnet restore
    
    - name: Build
      run: dotnet build benchmarks/HeroCsv.Benchmarks/HeroCsv.Benchmarks.csproj -c Release --no-restore
    
    - name: Run benchmarks
      run: |
        cd benchmarks/HeroCsv.Benchmarks
        # Determine which benchmark to run
        if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
          BENCHMARK_TYPE="${{ github.event.inputs.benchmark_type }}"
        else
          BENCHMARK_TYPE="quick"
        fi
        
        echo "Running $BENCHMARK_TYPE benchmarks..."
        dotnet run -c Release --no-build -- $BENCHMARK_TYPE
        
        # Find and copy JSON results to standard location for github-action-benchmark
        echo "Looking for benchmark results..."
        if [ -d "BenchmarkResults/BenchmarkDotNet/${BENCHMARK_TYPE}/results" ]; then
          echo "Found results directory"
          cp BenchmarkResults/BenchmarkDotNet/${BENCHMARK_TYPE}/results/*-report-brief.json benchmark-results.json || true
          if [ -f benchmark-results.json ]; then
            echo "Benchmark results saved to benchmark-results.json"
          fi
        fi
    
    - name: Find benchmark results
      if: always()
      run: |
        echo "Looking for benchmark results..."
        find . -name "*.json" -o -name "*.csv" -o -name "*.html" -o -name "*.md" | grep -i benchmark || true
        
        # Check if benchmark-results.json was created
        if [ -f benchmarks/HeroCsv.Benchmarks/benchmark-results.json ]; then
          echo "Found benchmark-results.json for comparison"
          ls -la benchmarks/HeroCsv.Benchmarks/benchmark-results.json
        fi
        
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: benchmark-results-${{ github.sha }}
        path: |
          BenchmarkResults/**/*
          benchmarks/HeroCsv.Benchmarks/bin/Release/**/*.json
          benchmarks/HeroCsv.Benchmarks/bin/Release/**/*.html
        retention-days: 90
    
    # Store benchmark results for comparison
    - name: Store benchmark result
      if: github.event_name == 'push' && github.ref == 'refs/heads/master'
      uses: benchmark-action/github-action-benchmark@v1
      with:
        tool: 'benchmarkdotnet'
        output-file-path: benchmarks/HeroCsv.Benchmarks/benchmark-results.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        gh-pages-branch: gh-pages
        benchmark-data-dir-path: 'benchmarks'
        auto-push: true
        comment-on-alert: true
        alert-threshold: '110%' # Alert if performance degrades by more than 10%
        fail-on-alert: false # Don't fail the build on performance regression
        summary-always: true
    
    # Compare PR benchmarks against master
    - name: Compare benchmark results
      if: github.event_name == 'pull_request'
      uses: benchmark-action/github-action-benchmark@v1
      with:
        tool: 'benchmarkdotnet'
        output-file-path: benchmarks/HeroCsv.Benchmarks/benchmark-results.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        gh-pages-branch: gh-pages
        benchmark-data-dir-path: 'benchmarks'
        alert-threshold: '105%' # More sensitive for PRs
        comment-on-alert: true
        fail-on-alert: false # Don't fail PRs, just warn
        summary-always: true
        comment-always: true # Always comment on PRs with benchmark results