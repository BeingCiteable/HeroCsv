name: Performance Benchmarks

on:
  pull_request:
    paths:
      - 'src/**'
      - 'benchmarks/**'
      - '.github/workflows/benchmarks.yml'
  push:
    branches: [master]
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: false
        default: 'quick'
        type: choice
        options:
          - quick
          - features
          - competitors
          - realdata
          - library
          - perf

env:
  DOTNET_VERSION: '9.0.x'
  DOTNET_NOLOGO: true
  DOTNET_CLI_TELEMETRY_OPTOUT: true

jobs:
  benchmark:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup .NET
      uses: actions/setup-dotnet@v4
      with:
        dotnet-version: ${{ env.DOTNET_VERSION }}
    
    - name: Restore dependencies
      run: dotnet restore
    
    - name: Build
      run: dotnet build benchmarks/HeroCsv.Benchmarks/HeroCsv.Benchmarks.csproj -c Release --no-restore
    
    - name: Run benchmarks
      id: run-benchmarks
      run: |
        cd benchmarks/HeroCsv.Benchmarks
        # Determine which benchmark to run
        if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
          BENCHMARK_TYPE="${{ github.event.inputs.benchmark_type }}"
        else
          BENCHMARK_TYPE="quick"
        fi
        
        echo "Running $BENCHMARK_TYPE benchmarks..."
        dotnet run -c Release --no-build -- $BENCHMARK_TYPE
        
        # Wait a moment for files to be written
        sleep 2
        
        # Find and process benchmark results
        echo "Looking for benchmark results..."
        
        # Find the most recent JSON report
        JSON_FILE=$(find . -name "*-report-brief.json" -type f -printf '%T@ %p\n' 2>/dev/null | sort -rn | head -1 | cut -d' ' -f2-)
        
        if [ -z "$JSON_FILE" ]; then
          # Try alternative search
          JSON_FILE=$(find . -name "*Benchmarks-report*.json" -type f | head -1)
        fi
        
        if [ -n "$JSON_FILE" ] && [ -f "$JSON_FILE" ]; then
          echo "Found JSON file: $JSON_FILE"
          cp "$JSON_FILE" benchmark-results.json
          echo "File size: $(stat -c%s benchmark-results.json 2>/dev/null || stat -f%z benchmark-results.json 2>/dev/null || echo 'unknown') bytes"
          echo "json_found=true" >> $GITHUB_OUTPUT
        else
          echo "WARNING: No JSON benchmark results found"
          echo "Creating placeholder results file"
          # Create a minimal valid BenchmarkDotNet JSON structure
          echo '{
            "Title": "Placeholder",
            "HostEnvironmentInfo": {},
            "Benchmarks": []
          }' > benchmark-results.json
          echo "json_found=false" >> $GITHUB_OUTPUT
        fi
        
        # Always ensure the file exists
        if [ ! -f benchmark-results.json ]; then
          echo '{}' > benchmark-results.json
        fi
    
    - name: Verify benchmark results
      if: always()
      run: |
        echo "Looking for benchmark results..."
        echo "Current directory: $(pwd)"
        
        if [ -f benchmarks/HeroCsv.Benchmarks/benchmark-results.json ]; then
          echo "✅ Found benchmark-results.json for comparison"
          ls -la benchmarks/HeroCsv.Benchmarks/benchmark-results.json
          echo "First 500 chars of JSON:"
          head -c 500 benchmarks/HeroCsv.Benchmarks/benchmark-results.json
        else
          echo "❌ benchmark-results.json not found"
          echo "Files in benchmarks/HeroCsv.Benchmarks:"
          ls -la benchmarks/HeroCsv.Benchmarks/ | head -20
        fi
        
        echo "All JSON files found:"
        find . -name "*.json" | grep -i benchmark | head -20 || true
        
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: benchmark-results-${{ github.sha }}
        path: |
          BenchmarkResults/**/*
          benchmarks/HeroCsv.Benchmarks/bin/Release/**/*.json
          benchmarks/HeroCsv.Benchmarks/bin/Release/**/*.html
        retention-days: 90
    
    # Store benchmark results for comparison (only if JSON was found)
    - name: Store benchmark result
      if: github.event_name == 'push' && github.ref == 'refs/heads/master' && steps.run-benchmarks.outputs.json_found == 'true'
      continue-on-error: true
      uses: benchmark-action/github-action-benchmark@v1
      with:
        tool: 'benchmarkdotnet'
        output-file-path: benchmarks/HeroCsv.Benchmarks/benchmark-results.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        gh-pages-branch: gh-pages
        benchmark-data-dir-path: 'benchmarks'
        auto-push: true
        comment-on-alert: true
        alert-threshold: '110%' # Alert if performance degrades by more than 10%
        fail-on-alert: false # Don't fail the build on performance regression
        summary-always: true
        skip-fetch-gh-pages: false
        max-items-in-chart: 30
    
    # Compare PR benchmarks against master (only if JSON was found)
    - name: Compare benchmark results
      if: github.event_name == 'pull_request' && steps.run-benchmarks.outputs.json_found == 'true'
      continue-on-error: true
      uses: benchmark-action/github-action-benchmark@v1
      with:
        tool: 'benchmarkdotnet'
        output-file-path: benchmarks/HeroCsv.Benchmarks/benchmark-results.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        gh-pages-branch: gh-pages
        benchmark-data-dir-path: 'benchmarks'
        alert-threshold: '105%' # More sensitive for PRs
        comment-on-alert: true
        fail-on-alert: false # Don't fail PRs, just warn
        summary-always: true
        comment-always: true # Always comment on PRs with benchmark results
        skip-fetch-gh-pages: false
        external-data-json-path: ./benchmark-data.json